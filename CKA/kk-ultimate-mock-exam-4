1. One application, webpage-server-01, is deployed on the Kubernetes cluster by the Helm tool. Now, the team wants to deploy a new version of the application by replacing the existing one. A new version of the helm chart is given in the /root/new-version directory on the cluster1-controlplane. Validate the chart before installing it on the Kubernetes cluster.
Use the helm command to validate and install the chart. After successfully installing the newer version, uninstall the older version.

In this task, we will use the helm commands. Here are the steps: -


Use the helm ls command to list the release deployed on the default namespace using helm.
helm ls -n default

First, validate the helm chart by using the helm lint command: -
cd /root/

helm lint ./new-version

Now, install the new version of the application by using the helm install command as follows: -
helm install --generate-name ./new-version

We haven't got any release name in the task, so we can generate the random name from the --generate-name option.

Finally, uninstall the old version of the application by using the helm uninstall command: -

helm uninstall webpage-server-01 -n default

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Solve this question on: ssh cluster3-controlplane


Part I:

Create a ClusterIP service, .i.e. ,service-3421-svcn in the spectra-1267 ns, which should expose the pods, namely, pod-23 and pod-21, with port set to 8080 and targetport to 80.

Part II:

Store the pod names and their IP addresses from the spectra-1267 ns at /root/pod_ips_cka05_svcn where the output is sorted by their IPs.

Please ensure the format as shown below:

POD_NAME        IP_ADDR
pod-1           ip-1
pod-3           ip-2
pod-2           ip-3
...



k get pods -o custom-columns=POD_NAME:.metadata.name,IP_ADDR=.ststus.ipaddress
kubectl -n admin2406 get deployment -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Solve this question on: ssh cluster3-controlplane


Utilize helm to search for the repository URL of the Bitnami version of the nginx repository. Ensure that you save the repository URL in the file located at /root/nginx-helm-url.txt on the cluster3-controlplane.

helm search hub nginx --list-repo-url | head -n15

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Solve this question on: ssh cluster2-controlplane


Identify the CPU and memory resource capacity on cluster2-node01 node and save the results in /root/cluster2-node01-cpu.txt and /root/cluster2-node01-memory.txt, respectively, on the cluster2-controlplane.

Store the values in the following format:

<Resource-name>: <Value>

kubectl describe node cluster2-node01

echo "cpu: 16" > cluster2-node01-cpu.txt
echo "memory: 65838280Ki" > cluster2-node01-memory.txt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Solve this question on: ssh cluster1-controlplane


A pod called check-time-cka03-trb is continuously crashing. Figure out what is causing this and fix it.


Ensure that the check-time-cka03-trb POD is in the running state.

This pod prints the current date and time at a pre-defined frequency and saves it to a file. Ensure that it continues this operation once you have fixed it.

logs we are getting oci error -  becaaue of bin/bash ---> bin/sh 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An etcd backup is already stored at the path /opt/cluster1_backup_to_restore.db on the cluster1-controlplane node. Use /root/default.etcd as the --data-dir and restore it on the cluster1-controlplane node itself.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The CoreDNS configuration in the cluster needs to be updated:

Update the CoreDNS configuration in the cluster so that DNS resolution for cka.local works exactly like cluster.local and in addition to it.

Test your configuration using the jrecord/nettools image by executing the following commands:

nslookup kubernetes.default.svc.cluster.local
nslookup kubernetes.default.svc.cka.local


1. Update the CoreDNS Configuration
The CoreDNS configuration is stored in a ConfigMap. To edit this configuration, use the following command:

kubectl edit cm -n kube-system coredns

exactly this line - 

kubernetes cka.local cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }

kubectl rollout restart deploy -n kube-system coredns

kubectl run test --rm -it --image=jrecord/nettools --restart=Never -- nslookup kubernetes.default.svc.cluster.local
kubectl run test --rm -it --image=jrecord/nettools --restart=Never -- nslookup kubernetes.default.svc.cka.local



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A debian package for cri-dockerd cri-dockerd_0.3.16.3-0.ubuntu-jammy_amd64.deb is located in the /root folder on the cluster5-controlplane. As part of cluster initialization, 
install the package and make sure that cri-docker service is up and enabled on the system. Additionally, enable IP forwarding on the server.

1. Install cri-dockerd
The Debian package for cri-dockerd is available at cri-dockerd_0.3.16.3-0.ubuntu-jammy_amd64.deb. Install it using the following command:

dpkg -i cri-dockerd_0.3.16.3-0.ubuntu-jammy_amd64.deb 

2. Enable and Start the Service
To enable and start the cri-dockerd service, use the following command:

sudo systemctl enable --now cri-docker.service

3. Enable IP Forwarding
To ensure that IP forwarding changes are persistent:

Create a configuration file:
   vi /etc/sysctl.d/k8s.conf

Add the following line to the file:
   net.ipv4.ip_forward=1

Apply the changes:
   sysctl -p

To set the changes temporarily, execute the following command:

sysctl -w net.ipv4.ip_forward=1


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Utilize the official flannel definition file, located at https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml, 
to deploy the flannel CNI on the cluster using the CIDR 172.17.0.0/16. Ensure pods can communicate after the CNI is installed.


1. Install the Flannel CNI
Download the Flannel manifest file:
wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

Edit the Flannel manifest to set the CIDR to 172.17.0.0/16:
vi kube-flannel.yml

Below is the structure of the CoreDNS configuration:

apiVersion: v1
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "172.17.0.0/16",   #Edited
      "EnableNFTables": false,
      "Backend": {
        "Type": "vxlan"
      }
    }
kind: ConfigMap
metadata:
  labels:
    app: flannel
    k8s-app: flannel
    tier: node
  name: kube-flannel-cfg
  namespace: kube-flannel

Apply the manifest by running the following command:
kubectl apply -f kube-flannel.yml

2. Test the pod-to-pod communication.
Run an nginx image:
kubectl run web-app --image nginx

Retrieve the IP address of the pod:
kubectl get pod web-app -o jsonpath='{.status.podIP}'

Test the connection:
kubectl run test --rm -it -n kube-public --image=jrecord/nettools --restart=Never -- curl <IP>

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We have a deployment named external-app and a service associated with it named external-service in the external namespace already created.

There is also an existing Ingress resource named external-ingress in the external namespace.

Create HTTPRoute to achieve the same behavior as external-ingress.

Part 1

Create an HTTP Gateway named web-gateway in the nginx-gateway namespace, which uses the nginx gateway class, and listens on port 80, and allows routes from all namespaces.
Part 2

Create an HTTPRoute named external-route in the external namespace that binds to the web-gateway in the nginx-gateway namespace and routes traffic to the external-service in the external namespace.
Part 3

Remove the existing Ingress resource from the external namespace.
Hit curl on the gateway to test the service.

# This is the node port where gatewayclass is attached to
curl localhost:30080

Solution
SSH into the cluster1-controlplane host
ssh cluster1-controlplane

Step 1: Use the GatewayClass named nginx and create a Gateway in the nginx-gateway namespace
Create a file named nginx-gateway.yaml:

# Create the Gateway
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All

Note: The allowedRoutes.namespaces.from: All setting explicitly allows routes from all namespaces to bind to this Gateway, which is required for our cross-namespace routing scenario.

Apply it:

kubectl apply -f nginx-gateway.yaml

Step 2: Create the HTTPRoute in the external namespace
First, review the existing ingress configuration:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
  creationTimestamp: "2025-03-24T05:21:34Z"
  generation: 1
  name: external-ingress
  namespace: external
  resourceVersion: "6592"
  uid: 2ec75a22-fa6d-45b8-90d8-29273a8fbd0c
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: external-service
            port:
              number: 80
        path: /
        pathType: Prefix

The ingress configuration routes all traffic to the external-service on port 80.

Now, create a file named external-route.yaml with the following content:

apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: external-route
  namespace: external
spec:
  parentRefs:
  - name: web-gateway
    namespace: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: external-service
      port: 80

Then, apply the HTTPRoute configuration:

kubectl apply -f external-route.yaml

Step 3: Remove the existing ingress resource
To delete the existing ingress resource, execute the following command:

kubectl delete -n external ingress external-ingress 

Step 4: Verify that the HTTPRoute is created and configured correctly
Describe the HTTPRoute to see its configuration:

kubectl describe httproute external-route -n external

Step 5: Test the routing functionality
Get the external IP of the Gateway:

kubectl get gateway web-gateway -n nginx-gateway

Test accessing the service using curl:

curl localhost:30080

You should see traffic being routed to the external-service in the external namespace.
